{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Handling Imbalanced Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simple terms, in binary classifiers , Imbalance of data is dominance of one class over other of **target variable**.\n",
    "\n",
    "Class Imbalance occurs in datasets pertaining multiclass classification as well.\n",
    "\n",
    "Imbalance datasets are characterized by a rare class, which represents a small portion of the entire population (1 out of 1000 or 1 out of 10000 or even more). Class imbalance can be intrinsic to the problem, it is imbalanced by its own nature, or it can be determined by the limitation of data collection, caused by economic or privacy reasons.\n",
    "\n",
    "The minority class is scarce and its own characteristics and own patterns are scarce as well, but those information is extremely important for the trained model to discriminate the small samples from the crowd. \n",
    "\n",
    "<hr>\n",
    "\n",
    "In Machine Learning and Data Science we often come across a term called Imbalanced Data Distribution, generally happens when observations in one of the class are much higher or lower than the other classes. As Machine Learning algorithms tend to increase accuracy by reducing the error, they do not consider the class distribution. This problem is prevalent in examples such as Fraud Detection, Anomaly Detection, Facial recognition etc.\n",
    "\n",
    "Standard ML techniques such as Decision Tree and Logistic Regression have a bias towards the majority class, and they tend to ignore the minority class. They tend only to predict the majority class, hence, having major misclassification of the minority class in comparison with the majority class. In more technical words, if we have imbalanced data distribution in our dataset then our model becomes more prone to the case when minority class has negligible or very lesser recall.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example**\n",
    "\n",
    "- User churn in telecom industry.\n",
    "\n",
    "- Detection of rare diseases.\n",
    "\n",
    "- Detecting Credit card Fradulant transactions.\n",
    "\n",
    "In above examples you can notice that number of data points of right class are very less as compared to other.\n",
    "\n",
    "**Balanced Data**\n",
    "\n",
    "<img src = 'a_img.png'>\n",
    "\n",
    "**Imbalanced Data**\n",
    "\n",
    "<img src = 'b_img.png'>\n",
    "\n",
    "Suppose to train a machine learning model to discern non-spam emails from spam emails. The entire dataset is composed of 44 emails, including 40 non-spam emails and 4 spam emails. The model used is a standard algorithm and doesnâ€™t take into account the class distribution. The result achieved is the following:\n",
    "\n",
    "<img src = 'f_img.png'>\n",
    "\n",
    "The model obtains 90.9% of accuracy. Great result! But is it a good model? Obviously not! The model acts like a Zero Rule model: only the majority class is found, while the rare class, that is more interesting, is ignored.\n",
    "\n",
    "Accuracy evaluates all the classes as equally important and thatâ€™s why it canâ€™t be used as measure of goodness for models working on imbalanced class dataset.\n",
    "Other metrics are necessary, such as:\n",
    "\n",
    "-    Recall\n",
    "-    Precision\n",
    "-    F1 Measure\n",
    "-    ROC curve and AUC\n",
    "\n",
    "Which one is better? There isnâ€™t a better metric. It depends on many factors, such as the goal, the context, and the cost function: is it better to classify correctly one more unit of the rare class but, at the same time, increasing False Positive errors (classify no-spam email as spam email), or misclassify some units of the rare class, but decreasing False Positive errors?\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ways to Handle Imbalance data**\n",
    "\n",
    "### **1. Change Another Algorithm & Model ML**\n",
    "### **2. Change Metrics**\n",
    "### **3. Re-sampling techniques**\n",
    "ðŸ“Œ **Natural Resampling**: the main goal is to collect more data of the minority class. Itâ€™s simple to obtain, but itâ€™s not always possible. In dataset where the imbalance problem is part of its own nature, itâ€™s hard to collect as data as the majority class.\n",
    "\n",
    "ðŸ“Œ **Artificial Resampling**: it can be accomplished by:\n",
    "\n",
    "-    **Undersampling**, by reducing data of the majority class\n",
    "-    **Oversampling**, by replicating the minority class\n",
    "-    **SMOTE** (Synthetic Minority Oversampling TEchnique). It is a synthetic minority oversampling technique, which makes synthetic data points by finding the nearest neighbours to each minority sample.\n",
    "- **NearMiss Algorithm** is an under-sampling technique. It aims to balance class distribution by randomly eliminating majority class examples. \n",
    "\n",
    "<hr>\n",
    "\n",
    "In re-sampling of data either we reduce the proportion of dominant class which is under-sampling or we increase proportion of minority class which is called as oversampling.\n",
    "\n",
    "However most successful approaches uses both oversampling and under-sampling together.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1. Under-sampling**\n",
    "Undersampling can cut out some important and valuable information from the dataset. To overcome the loss of information caused by undersampling, a cluster-based undersampling approach can applied. It uses the k-means alghoritm: the clusters created have the smaller variance within and the most between-class variance, thus the records inside the cluster share the same characteristics. The majority class is undersampled by taking only the centroids of the clusters created.\n",
    "\n",
    "<img src = 'c_img.png'>\n",
    "\n",
    "### **3.1.1 Random Under-sampling**\n",
    "\n",
    "Random under-sampling is very simple and intuitive under- sampling technique . Method works by randomly choosing the samples from dominant class.\n",
    "\n",
    "There are two major drawbacks of these technique:\n",
    "\n",
    "1. Major drawback of this technique is that we eliminate samples randomly, which may lead to loss of potential information .\n",
    "\n",
    "2. The purpose of machine learning is for the classifier to estimate the probability distribution of the target population. Since that distribution is unknown we try to estimate the population distribution using a sample distribution. Statistics tells us that as long as the sample is drawn randomly, the sample distribution can be used to estimate the population distribution from where it was drawn.\n",
    "\n",
    "### **3.1.2 Cluster centroid under-sampling**\n",
    "\n",
    "clusters of majority class and replace that cluster with centroid of that cluster. So we undersample majority class by forming clusters and replacing it with cluster centroids.\n",
    "\n",
    "For example:\n",
    "- Majority class : 100 samples\n",
    "- Minority class : 10 samples\n",
    "\n",
    "Here , in this case we can form 10 clusters of majority class and replace 100 points with 10 data points i.e by cluster centroid.\n",
    "\n",
    "### **3.1.3 Under-sampling using Tomek links**\n",
    "\n",
    "Tomek link pair has two opposite class data points who are their own nearest neighbors. Main idea is to separate minority and majority class.\n",
    "\n",
    "Suppose,\n",
    "- d(A,B) : distance between two data points A & B\n",
    "- Then, a(A,B) is Tomek link if and only if\n",
    "- There is no such point â€˜Câ€™ , such that, d(A,C) < d(A<B) or d(B,C) < d(A,B)\n",
    "\n",
    "If pair of samples form tomek link then either one of the sample is noise or both are placed at border.\n",
    "\n",
    "As under-sampling technique we eliminate majority class point , while as part of data mining we eliminate both points.\n",
    "\n",
    "### **3.1.4 NearMiss Algorithm**\n",
    "\n",
    "NearMiss is an under-sampling technique. It aims to balance class distribution by randomly eliminating majority class examples. When instances of two different classes are very close to each other, we remove the instances of the majority class to increase the spaces between the two classes. This helps in the classification process.\n",
    "\n",
    "To prevent problem of information loss in most under-sampling techniques, near-neighbor methods are widely used.\n",
    "\n",
    "The basic intuition about the working of near-neighbor methods is as follows:\n",
    "- **Step 1**: The method first finds the distances between all instances of the majority class and the instances of the minority class. Here, majority class is to be under-sampled.\n",
    "\n",
    "- **Step 2**: Then, n instances of the majority class that have the smallest distances to those in the minority class are selected.\n",
    "\n",
    "- **Step 3**: If there are k instances in the minority class, the nearest method will result in k*n instances of the majority class.\n",
    "\n",
    "For finding n closest instances in the majority class, there are several variations of applying NearMiss Algorithm :\n",
    "\n",
    "-    NearMiss â€“ Version 1 : It selects samples of the majority class for which average distances to the k closest instances of the minority class is smallest.\n",
    "-    NearMiss â€“ Version 2 : It selects samples of the majority class for which average distances to the k farthest instances of the minority class is smallest.\n",
    "-    NearMiss â€“ Version 3 : It works in 2 steps. Firstly, for each minority class instance, their M nearest-neighbors will be stored. Then finally, the majority class instances are selected for which the average distance to the N nearest-neighbors is the largest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2. Over-sampling**\n",
    "Oversampling, on the other hand, can lead to overfitting. Although, itâ€™s been proved that adjusting the class distribution to the optimal one can improve drastically the performance, but find the best distribution is really difficult. Some dataset are more reactable to fully balanced distribution class, other instead gets greater performance with less skewed dataset. The researcher should find the better solution by trial and error and some heuristics\n",
    "\n",
    "<img src = 'd_img.png'>\n",
    "\n",
    "### **3.2.1. Random Oversampling**\n",
    "\n",
    "In random oversampling technique we replace the samples with existing minority samples. In simple terms we can say that we just do multiplication of existing Minority class.\n",
    "\n",
    "Major drawback of this technique us that , this technique is highly prone to over fitting.\n",
    "\n",
    "### **3.2.2. SMOTE (Synthetic Minority Oversampling Technique)**\n",
    "\n",
    "In this technique we calculate difference between sample under consideration and its nearest neighbors. Once the distance is calculated we multiply that with the number between 0 and 1. We add it to sample under consideration.\n",
    "\n",
    "Which gives us new sample point for minority class .\n",
    "\n",
    "Depending upon the amount of oversampling required , neighbours from k-NN are randomly chosen.\n",
    "\n",
    "<img src = 'e_img.png'>\n",
    "\n",
    "SMOTE (synthetic minority oversampling technique) is one of the most commonly used oversampling methods to solve the imbalance problem.\n",
    "It aims to balance class distribution by randomly increasing minority class examples by replicating them.\n",
    "\n",
    "SMOTE synthesises new minority instances between existing minority instances. It generates the virtual training records by linear interpolation for the minority class. These synthetic training records are generated by randomly selecting one or more of the k-nearest neighbors for each example in the minority class. After the oversampling process, the data is reconstructed and several classification models can be applied for the processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## **Credit Card Dataset**\n",
    "\n",
    "**Context**\n",
    "\n",
    "It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n",
    "\n",
    "**Content**\n",
    "\n",
    "The datasets contains transactions made by credit cards in September 2013 by european cardholders.\n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import confusion_matrix, classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# load the data set \n",
    "data = pd.read_csv('creditcard.csv') \n",
    "\n",
    "print(data.info()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalise the amount column \n",
    "data['normAmount'] = StandardScaler().fit_transform(np.array(data['Amount']).reshape(-1, 1)) \n",
    "  \n",
    "# drop Time and Amount (not relevant for prediction purpose)\n",
    "data = data.drop(['Time', 'Amount'], axis = 1) \n",
    "  \n",
    "# there are 492 fraud transactions. \n",
    "data['Class'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['Class'], axis=1)\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number transactions X_train dataset:  (227845, 29)\n",
      "Number transactions y_train dataset:  (227845,)\n",
      "Number transactions X_test dataset:  (56962, 29)\n",
      "Number transactions y_test dataset:  (56962,)\n"
     ]
    }
   ],
   "source": [
    "# split (80:20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) \n",
    "  \n",
    "# describes info about train and test set \n",
    "print(\"Number transactions X_train dataset: \", X_train.shape) \n",
    "print(\"Number transactions y_train dataset: \", y_train.shape) \n",
    "print(\"Number transactions X_test dataset: \", X_test.shape) \n",
    "print(\"Number transactions y_test dataset: \", y_test.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56861\n",
      "           1       0.88      0.63      0.74       101\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.94      0.82      0.87     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logistic regression object \n",
    "lr = LogisticRegression() \n",
    "  \n",
    "# train the model on train set \n",
    "lr.fit(X_train, y_train.ravel()) \n",
    "  \n",
    "predictions = lr.predict(X_test) \n",
    "  \n",
    "# print classification report \n",
    "print(classification_report(y_test, predictions)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using SMOTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': 391\n",
      "Before OverSampling, counts of label '0': 227454 \n",
      "\n",
      "After OverSampling, the shape of train_X: (454908, 29)\n",
      "After OverSampling, the shape of train_y: (454908,) \n",
      "\n",
      "After OverSampling, counts of label '1': 227454\n",
      "After OverSampling, counts of label '0': 227454\n"
     ]
    }
   ],
   "source": [
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n",
    "  \n",
    "# import SMOTE module from imblearn library \n",
    "from imblearn.over_sampling import SMOTE \n",
    "sm = SMOTE(random_state = 2) \n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel()) \n",
    "  \n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape)) \n",
    "  \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE Algorithm has oversampled the minority instances and made it equal to majority class. Both categories have equal amount of records. More specifically, the minority class has been increased to the total number of majority class.\n",
    "Now see the accuracy and recall results after applying SMOTE algorithm (Oversampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     56861\n",
      "           1       0.06      0.94      0.12       101\n",
      "\n",
      "    accuracy                           0.97     56962\n",
      "   macro avg       0.53      0.96      0.55     56962\n",
      "weighted avg       1.00      0.97      0.99     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr1 = LogisticRegression() \n",
    "lr1.fit(X_train_res, y_train_res.ravel()) \n",
    "predictions = lr1.predict(X_test) \n",
    "  \n",
    "# print classification report \n",
    "print(classification_report(y_test, predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We have reduced the accuracy to 98% as compared to previous model but the recall value of minority class has also improved to 92 %. This is a good model compared to the previous one. Recall is great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NearMiss Algorithm**\n",
    "\n",
    "NearMiss is an under-sampling technique. It aims to balance class distribution by randomly eliminating majority class examples. When instances of two different classes are very close to each other, we remove the instances of the majority class to increase the spaces between the two classes. This helps in the classification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Undersampling, counts of label '1': 391\n",
      "Before Undersampling, counts of label '0': 227454 \n",
      "\n",
      "After Undersampling, the shape of train_X: (782, 29)\n",
      "After Undersampling, the shape of train_y: (782,) \n",
      "\n",
      "After Undersampling, counts of label '1': 391\n",
      "After Undersampling, counts of label '0': 391\n"
     ]
    }
   ],
   "source": [
    "print(\"Before Undersampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
    "print(\"Before Undersampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n",
    "  \n",
    "# apply near miss \n",
    "from imblearn.under_sampling import NearMiss \n",
    "nr = NearMiss() \n",
    "  \n",
    "X_train_miss, y_train_miss = nr.fit_sample(X_train, y_train.ravel()) \n",
    "  \n",
    "print('After Undersampling, the shape of train_X: {}'.format(X_train_miss.shape)) \n",
    "print('After Undersampling, the shape of train_y: {} \\n'.format(y_train_miss.shape)) \n",
    "  \n",
    "print(\"After Undersampling, counts of label '1': {}\".format(sum(y_train_miss == 1))) \n",
    "print(\"After Undersampling, counts of label '0': {}\".format(sum(y_train_miss == 0))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NearMiss Algorithm has undersampled the majority instances and made it equal to majority class. Here, the majority class has been reduced to the total number of minority class, so that both classes will have equal number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.62      0.76     56861\n",
      "           1       0.00      0.95      0.01       101\n",
      "\n",
      "    accuracy                           0.62     56962\n",
      "   macro avg       0.50      0.78      0.39     56962\n",
      "weighted avg       1.00      0.62      0.76     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train the model on train set \n",
    "lr2 = LogisticRegression() \n",
    "lr2.fit(X_train_miss, y_train_miss.ravel()) \n",
    "predictions = lr2.predict(X_test) \n",
    "  \n",
    "# print classification report \n",
    "print(classification_report(y_test, predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is better than the first model because it classifies better and also the recall value of minority class is 95 %. But due to undersampling of majority class, its **recall has decreased to 56 %**.\n",
    "\n",
    "**SMOTE is better to use in this case.**\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Take Home Exercise**\n",
    "\n",
    "**1. Use any ML for Classification Model to predict Credit Fraud in this dataset + before & after re-sampling target:**\n",
    "- Logistic Regression (seperti contoh di atas)\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- KNN\n",
    "\n",
    "What's the best model in Precision / Recall Metric?\n",
    "\n",
    "**2. Use Hyperparameter tuning + after re-sampling target (SMOTE):**\n",
    "- **Logistic Regression** (Ivan, Gabriella, Taufik, Stevanus, Rifki, Aziz, Dicky, Ramzy)\n",
    "- **Decision Tree** (Dani, Rizky P., Rinta, Jihar, Cahya, Indra, Shabrina, Agung)\n",
    "- **Random Forest** (Dimas, Michael, Rizki L., Faris, Pradana, Imanta, Goldy, Fariz G.)\n",
    "\n",
    "What's the best model in Precision / Recall Metric?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reference**:\n",
    "- Aditya Patil, \"Dealing with Imbalance Data\", https://medium.com/@patiladitya81295/dealing-with-imbalance-data-1bacc7d68dff\n",
    "- Roberta Pollastro, \"How to handle Class Imbalance Problem\", https://medium.com/quantyca/how-to-handle-class-imbalance-problem-9ee3062f2499\n",
    "- Baptiste Rocca, \"Handling imbalanced datasets in machine learning\", https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28\n",
    "- Geeksforgeeks, \"ML | Handling Imbalanced Data with SMOTE and Near Miss Algorithm in Python\", https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/\n",
    "- Rahul Agarwal, \"The 5 most useful Techniques to Handle Imbalanced datasets\", https://towardsdatascience.com/the-5-most-useful-techniques-to-handle-imbalanced-datasets-6cdba096d55a\n",
    "- Dataset source: https://www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 32-bit",
   "language": "python",
   "name": "python38132bitf9f79e71b62e4503b25567c1d3914456"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
